{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5226b8d7-7dea-4340-8f1c-82d68c11284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28d4eb6b-988e-49ae-abc5-c7c3bb2929f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/l00pz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/l00pz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/l00pz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/l00pz/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/l00pz/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') \n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "nltk.download('punkt_tab') \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a28da610-8752-4a2f-acb4-df66e9852cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf4c4ff2-619c-4f97-bb7e-653441b7b94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization:\n",
      " ['Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language.', \"', 'In particular, it focuses on how to program computers to process and analyze large amounts of natural language data\"]\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence Tokenization:\\n\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb8633c2-fd55-46b9-a9b3-b26b8df9d1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Tokenization:\n",
      " ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', '.', \"'\", ',', \"'In\", 'particular', ',', 'it', 'focuses', 'on', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "print(\"\\nWord Tokenization:\\n\", words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30c2e4e2-1d92-4a51-af74-d70e746f92cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Removing Punctuation:\n",
      " Natural language processing  NLP  is a subfield of linguistics  computer science  and artificial intelligence concerned with the interactions between computers and human language     In particular  it focuses on how to program computers to process and analyze large amounts of natural language data\n"
     ]
    }
   ],
   "source": [
    "text_clean = re.sub('[^a-zA-Z]', ' ', text)\n",
    "print(\"\\nAfter Removing Punctuation:\\n\", text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09159d06-ff1b-45fa-a517-8e4020e637c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered Words:\n",
      " ['natural', 'language', 'processing', 'nlp', 'subfield', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'language', 'particular', 'focuses', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(text_clean.lower())\n",
    "filtered_words = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
    "print(\"\\nFiltered Words:\\n\", filtered_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f92d82f8-063f-43b2-8961-cc6307672187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemmed Words:\n",
      " ['natur', 'languag', 'process', 'nlp', 'subfield', 'linguist', 'comput', 'scienc', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', 'languag', 'particular', 'focus', 'program', 'comput', 'process', 'analyz', 'larg', 'amount', 'natur', 'languag', 'data']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in filtered_words]\n",
    "print(\"\\nStemmed Words:\\n\", stemmed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcbde45d-49ab-4b3a-8035-8f1d008675cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatized Words:\n",
      " ['natural', 'language', 'processing', 'nlp', 'subfield', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interaction', 'computer', 'human', 'language', 'particular', 'focus', 'program', 'computer', 'process', 'analyze', 'large', 'amount', 'natural', 'language', 'data']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "print(\"\\nLemmatized Words:\\n\", lemmatized)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6567cadc-5783-4852-ab19-4597eff9d7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS Tags:\n",
      " [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('linguistics', 'NNS'), (',', ','), ('computer', 'NN'), ('science', 'NN'), (',', ','), ('and', 'CC'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('concerned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('interactions', 'NNS'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('human', 'JJ'), ('language', 'NN'), ('.', '.'), (\"'\", \"''\"), (',', ','), (\"'In\", \"''\"), ('particular', 'JJ'), (',', ','), ('it', 'PRP'), ('focuses', 'VBZ'), ('on', 'IN'), ('how', 'WRB'), ('to', 'TO'), ('program', 'NN'), ('computers', 'NNS'), ('to', 'TO'), ('process', 'VB'), ('and', 'CC'), ('analyze', 'VB'), ('large', 'JJ'), ('amounts', 'NNS'), ('of', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('data', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "pos_tags = nltk.pos_tag(words)\n",
    "print(\"\\nPOS Tags:\\n\", pos_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "025be922-5e68-4ef8-b524-065f57be64a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Representation:\n",
      "\n",
      "amount: 0.15811388300841897\n",
      "analyze: 0.15811388300841897\n",
      "artificial: 0.15811388300841897\n",
      "computer: 0.4743416490252569\n",
      "concerned: 0.15811388300841897\n",
      "data: 0.15811388300841897\n",
      "focus: 0.15811388300841897\n",
      "human: 0.15811388300841897\n",
      "intelligence: 0.15811388300841897\n",
      "interaction: 0.15811388300841897\n",
      "language: 0.4743416490252569\n",
      "large: 0.15811388300841897\n",
      "linguistics: 0.15811388300841897\n",
      "natural: 0.31622776601683794\n",
      "nlp: 0.15811388300841897\n",
      "particular: 0.15811388300841897\n",
      "process: 0.15811388300841897\n",
      "processing: 0.15811388300841897\n",
      "program: 0.15811388300841897\n",
      "science: 0.15811388300841897\n",
      "subfield: 0.15811388300841897\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([' '.join(lemmatized)])\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(\"\\nTF-IDF Representation:\\n\")\n",
    "for word, score in zip(feature_names, tfidf_matrix.toarray()[0]):\n",
    "    print(f\"{word}: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
